from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import os
import sys
import json
import glob

# Ajouter le rÃ©pertoire services au path
sys.path.insert(0, '/opt/airflow/services')

from data_validator import DataValidator, generate_validation_summary
from dashboard_collector import DashboardCollector


# Chemins
REPORTS_DIR = '/opt/airflow/data/validation_reports'
DATA_DIR = '/opt/airflow/data'


def validate_data_quality(**context):
    """TÃ¢che principale de validation de la qualitÃ© des donnÃ©es depuis PostgreSQL."""
    # CrÃ©er le rÃ©pertoire de rapports s'il n'existe pas
    os.makedirs(REPORTS_DIR, exist_ok=True)
    
    # Initialiser le collecteur pour accÃ©der Ã  la BDD
    collector = DashboardCollector(DATA_DIR)
    
    # RÃ©cupÃ©rer toutes les entreprises depuis la BDD
    print("ğŸ“Š RÃ©cupÃ©ration des entreprises depuis PostgreSQL...")
    session = collector.Session()
    try:
        from dashboard_collector import Entreprise
        entreprises_db = session.query(Entreprise).all()
        
        print(f"ğŸ“ Nombre d'entreprises Ã  valider: {len(entreprises_db)}")
        
        if len(entreprises_db) == 0:
            print("âš ï¸  Aucune entreprise trouvÃ©e dans la BDD")
            return {
                'status': 'no_data',
                'message': 'Aucune entreprise Ã  valider'
            }
        
        # Convertir les donnÃ©es JSONB en format compatible avec le validateur
        # Le validateur attend des donnÃ©es au format du parser HTML
        validation_data = []
        for entreprise in entreprises_db:
            # entreprise.data contient dÃ©jÃ  le dict complet au format attendu
            validation_data.append({
                'numero_entreprise': entreprise.numero_entreprise,
                'data': entreprise.data  # JSONB dÃ©jÃ  parsÃ© en dict Python
            })
        
    finally:
        session.close()
    
    # Initialiser le validateur
    validator = DataValidator()
    
    # Valider toutes les entreprises
    print("ğŸ” DÃ©marrage de la validation...")
    
    # Adapter la validation pour les donnÃ©es en mÃ©moire
    results = []
    total_files = len(validation_data)
    valid_count = 0
    error_types = {}
    error_locations = {}
    
    for item in validation_data:
        data = item['data']
        numero = item['numero_entreprise']
        
        try:
            validation = validator.validate_entity(data)
            results.append(validation)
            
            if validation['valide']:
                valid_count += 1
            
            # Compter les types d'erreurs et enregistrer les entreprises affectÃ©es
            for error in validation['erreurs']:
                error_types[error] = error_types.get(error, 0) + 1
                error_locations.setdefault(error, set()).add(numero)
                
        except Exception as e:
            err_msg = f'erreur_validation: {str(e)}'
            results.append({
                'entreprise': numero,
                'valide': False,
                'erreurs': [err_msg],
                'validation_date': datetime.now().isoformat()
            })
            error_types[err_msg] = error_types.get(err_msg, 0) + 1
            error_locations.setdefault(err_msg, set()).add(numero)
    
    # Calculer les statistiques
    invalid_count = total_files - valid_count
    valid_percentage = (valid_count / total_files * 100) if total_files > 0 else 0
    
    # Calculer les champs manquants
    missing_fields = {k: v for k, v in error_types.items() if 'manquant' in k}
    format_errors = {k: v for k, v in error_types.items() if 'format_invalide' in k or 'type_invalide' in k}
    
    missing_fields_percentage = sum(missing_fields.values()) / (total_files * len(validator.validation_rules['presentation'])) * 100 if total_files > 0 else 0
    
    report = {
        'date_validation': datetime.now().isoformat(),
        'source': 'PostgreSQL (table entreprises)',
        'statistiques': {
            'total_entreprises': total_files,
            'entreprises_valides': valid_count,
            'entreprises_invalides': invalid_count,
            'pourcentage_valides': round(valid_percentage, 2),
            'pourcentage_champs_manquants': round(missing_fields_percentage, 2)
        },
        'repartition_erreurs': error_types,
        'erreurs_localisation': {k: sorted(list(v)) for k, v in error_locations.items()},
        'champs_manquants': missing_fields,
        'erreurs_format': format_errors,
        'details_validations': results
    }
    
    # GÃ©nÃ©rer le rÃ©sumÃ© textuel
    summary = generate_validation_summary(report)
    print(summary)
    
    # Sauvegarder le rapport complet
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    report_file = os.path.join(REPORTS_DIR, f'validation_report_{timestamp}.json')
    
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ“„ Rapport complet sauvegardÃ©: {report_file}")
    
    # Sauvegarder aussi le dernier rapport (pour faciliter l'accÃ¨s)
    latest_report_file = os.path.join(REPORTS_DIR, 'latest_validation_report.json')
    with open(latest_report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    # Sauvegarder le rÃ©sumÃ© textuel
    summary_file = os.path.join(REPORTS_DIR, f'validation_summary_{timestamp}.txt')
    with open(summary_file, 'w', encoding='utf-8') as f:
        f.write(summary)
    
    # Pousser les statistiques dans XCom
    context['ti'].xcom_push(key='validation_stats', value=report['statistiques'])
    context['ti'].xcom_push(key='report_file', value=report_file)
    
    return report['statistiques']


def generate_dashboard_data(**context):
    """GÃ©nÃ¨re les donnÃ©es pour le dashboard admin."""
    stats = context['ti'].xcom_pull(task_ids='validate_quality', key='validation_stats')
    
    if not stats:
        print("âš ï¸  Aucune statistique disponible")
        return
    
    # CrÃ©er un fichier JSON pour le dashboard
    dashboard_data = {
        'last_update': datetime.now().isoformat(),
        'source': 'PostgreSQL',
        'metrics': {
            'total_entreprises': stats['total_entreprises'],
            'taux_validite': stats['pourcentage_valides'],
            'taux_erreurs': 100 - stats['pourcentage_valides'],
            'champs_manquants_pct': stats['pourcentage_champs_manquants']
        },
        'status': 'success' if stats['pourcentage_valides'] >= 80 else 'warning'
    }
    
    dashboard_file = os.path.join(REPORTS_DIR, 'dashboard_metrics.json')
    with open(dashboard_file, 'w', encoding='utf-8') as f:
        json.dump(dashboard_data, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ“Š MÃ©triques dashboard sauvegardÃ©es: {dashboard_file}")
    print(f"âœ“ Source: PostgreSQL (table entreprises)")
    print(f"âœ“ Taux de validitÃ©: {stats['pourcentage_valides']}%")
    print(f"âœ“ Taux d'erreurs: {100 - stats['pourcentage_valides']}%")
    
    return dashboard_data


def check_data_quality_threshold(**context):
    """VÃ©rifie que le seuil de qualitÃ© est respectÃ©."""
    stats = context['ti'].xcom_pull(task_ids='validate_quality', key='validation_stats')
    
    if not stats:
        raise ValueError("Aucune statistique de validation disponible")
    
    # Seuil de qualitÃ© : 80% de validitÃ© minimum
    quality_threshold = 80
    validity_rate = stats['pourcentage_valides']
    
    if validity_rate < quality_threshold:
        print(f"âš ï¸  ALERTE QUALITÃ‰: Taux de validitÃ© ({validity_rate}%) < seuil ({quality_threshold}%)")
        print(f"   Entreprises invalides: {stats['entreprises_invalides']}/{stats['total_entreprises']}")
        # Ne pas Ã©chouer la tÃ¢che, juste alerter
        return {
            'status': 'warning',
            'message': f"QualitÃ© en dessous du seuil: {validity_rate}% < {quality_threshold}%"
        }
    else:
        print(f"âœ“ QualitÃ© OK: {validity_rate}% >= {quality_threshold}%")
        return {
            'status': 'success',
            'message': f"QualitÃ© satisfaisante: {validity_rate}%"
        }


# Configuration du DAG
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'kbo_data_quality_validation',
    default_args=default_args,
    description='Validation de la qualitÃ© des donnÃ©es extraites avec rapport dÃ©taillÃ©',
    schedule='0 6 * * *',  # Tous les jours Ã  6h00 (aprÃ¨s le scraping)
    start_date=datetime(2025, 1, 1),
    catchup=False,
    tags=['kbo', 'quality', 'validation'],
)

# TÃ¢che 1: Valider la qualitÃ© des donnÃ©es
validate_task = PythonOperator(
    task_id='validate_quality',
    python_callable=validate_data_quality,
    dag=dag,
)

# TÃ¢che 2: GÃ©nÃ©rer les donnÃ©es du dashboard
dashboard_task = PythonOperator(
    task_id='generate_dashboard',
    python_callable=generate_dashboard_data,
    dag=dag,
)

# TÃ¢che 3: VÃ©rifier le seuil de qualitÃ©
check_threshold_task = PythonOperator(
    task_id='check_threshold',
    python_callable=check_data_quality_threshold,
    dag=dag,
)

# DÃ©finir l'ordre des tÃ¢ches
validate_task >> dashboard_task >> check_threshold_task
