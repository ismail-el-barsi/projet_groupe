from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import os
import sys
import json
import glob

# Ajouter le rÃ©pertoire services au path
sys.path.insert(0, '/opt/airflow/services')

from data_validator import DataValidator, generate_validation_summary


# Chemins
JSON_DIR = '/opt/airflow/data/extracted_data'
REPORTS_DIR = '/opt/airflow/data/validation_reports'


def validate_data_quality(**context):
    """TÃ¢che principale de validation de la qualitÃ© des donnÃ©es."""
    # CrÃ©er le rÃ©pertoire de rapports s'il n'existe pas
    os.makedirs(REPORTS_DIR, exist_ok=True)
    
    # Lister tous les fichiers JSON (exclure les rapports)
    json_pattern = os.path.join(JSON_DIR, '*.json')
    json_files = [f for f in glob.glob(json_pattern) if 'report' not in f.lower()]
    
    print(f"ğŸ“ Nombre de fichiers JSON Ã  valider: {len(json_files)}")
    
    if len(json_files) == 0:
        print("âš ï¸  Aucun fichier JSON trouvÃ© Ã  valider")
        return {
            'status': 'no_files',
            'message': 'Aucun fichier Ã  valider'
        }
    
    # Initialiser le validateur
    validator = DataValidator()
    
    # Valider tous les fichiers
    print("ğŸ” DÃ©marrage de la validation...")
    report = validator.validate_all(json_files)
    
    # GÃ©nÃ©rer le rÃ©sumÃ© textuel
    summary = generate_validation_summary(report)
    print(summary)
    
    # Sauvegarder le rapport complet
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    report_file = os.path.join(REPORTS_DIR, f'validation_report_{timestamp}.json')
    
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ“„ Rapport complet sauvegardÃ©: {report_file}")
    
    # Sauvegarder aussi le dernier rapport (pour faciliter l'accÃ¨s)
    latest_report_file = os.path.join(REPORTS_DIR, 'latest_validation_report.json')
    with open(latest_report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    # Sauvegarder le rÃ©sumÃ© textuel
    summary_file = os.path.join(REPORTS_DIR, f'validation_summary_{timestamp}.txt')
    with open(summary_file, 'w', encoding='utf-8') as f:
        f.write(summary)
    
    # Pousser les statistiques dans XCom
    context['ti'].xcom_push(key='validation_stats', value=report['statistiques'])
    context['ti'].xcom_push(key='report_file', value=report_file)
    
    return report['statistiques']


def generate_dashboard_data(**context):
    """GÃ©nÃ¨re les donnÃ©es pour le dashboard admin."""
    stats = context['ti'].xcom_pull(task_ids='validate_quality', key='validation_stats')
    
    if not stats:
        print("âš ï¸  Aucune statistique disponible")
        return
    
    # CrÃ©er un fichier JSON pour le dashboard
    dashboard_data = {
        'last_update': datetime.now().isoformat(),
        'metrics': {
            'total_entreprises': stats['total_entreprises'],
            'taux_validite': stats['pourcentage_valides'],
            'taux_erreurs': 100 - stats['pourcentage_valides'],
            'champs_manquants_pct': stats['pourcentage_champs_manquants']
        },
        'status': 'success' if stats['pourcentage_valides'] >= 80 else 'warning'
    }
    
    dashboard_file = os.path.join(REPORTS_DIR, 'dashboard_metrics.json')
    with open(dashboard_file, 'w', encoding='utf-8') as f:
        json.dump(dashboard_data, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ“Š MÃ©triques dashboard sauvegardÃ©es: {dashboard_file}")
    print(f"âœ“ Taux de validitÃ©: {stats['pourcentage_valides']}%")
    print(f"âœ“ Taux d'erreurs: {100 - stats['pourcentage_valides']}%")
    
    return dashboard_data


def check_data_quality_threshold(**context):
    """VÃ©rifie que le seuil de qualitÃ© est respectÃ©."""
    stats = context['ti'].xcom_pull(task_ids='validate_quality', key='validation_stats')
    
    if not stats:
        raise ValueError("Aucune statistique de validation disponible")
    
    # Seuil de qualitÃ© : 80% de validitÃ© minimum
    quality_threshold = 80
    validity_rate = stats['pourcentage_valides']
    
    if validity_rate < quality_threshold:
        print(f"âš ï¸  ALERTE QUALITÃ‰: Taux de validitÃ© ({validity_rate}%) < seuil ({quality_threshold}%)")
        print(f"   Entreprises invalides: {stats['entreprises_invalides']}/{stats['total_entreprises']}")
        # Ne pas Ã©chouer la tÃ¢che, juste alerter
        return {
            'status': 'warning',
            'message': f"QualitÃ© en dessous du seuil: {validity_rate}% < {quality_threshold}%"
        }
    else:
        print(f"âœ“ QualitÃ© OK: {validity_rate}% >= {quality_threshold}%")
        return {
            'status': 'success',
            'message': f"QualitÃ© satisfaisante: {validity_rate}%"
        }


# Configuration du DAG
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'kbo_data_quality_validation',
    default_args=default_args,
    description='Validation de la qualitÃ© des donnÃ©es extraites avec rapport dÃ©taillÃ©',
    schedule='0 6 * * *',  # Tous les jours Ã  6h00 (aprÃ¨s le scraping)
    start_date=datetime(2025, 1, 1),
    catchup=False,
    tags=['kbo', 'quality', 'validation'],
)

# TÃ¢che 1: Valider la qualitÃ© des donnÃ©es
validate_task = PythonOperator(
    task_id='validate_quality',
    python_callable=validate_data_quality,
    dag=dag,
)

# TÃ¢che 2: GÃ©nÃ©rer les donnÃ©es du dashboard
dashboard_task = PythonOperator(
    task_id='generate_dashboard',
    python_callable=generate_dashboard_data,
    dag=dag,
)

# TÃ¢che 3: VÃ©rifier le seuil de qualitÃ©
check_threshold_task = PythonOperator(
    task_id='check_threshold',
    python_callable=check_data_quality_threshold,
    dag=dag,
)

# DÃ©finir l'ordre des tÃ¢ches
validate_task >> dashboard_task >> check_threshold_task
