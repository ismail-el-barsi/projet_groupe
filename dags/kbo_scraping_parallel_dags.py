"""
10 DAGs simples - Chaque DAG scrape 1 entreprise, puis se relance automatiquement
"""
import csv
import json
import os
import sys
from datetime import datetime, timedelta

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.trigger_dagrun import TriggerDagRunOperator

# Ajouter les chemins
dag_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(dag_dir)
services_dir = os.path.join(parent_dir, 'services')

if services_dir not in sys.path:
    sys.path.insert(0, services_dir)
if parent_dir not in sys.path:
    sys.path.insert(0, parent_dir)

from fetch_proxies import fetch_all_proxies
from kbo_scraper import KBOScraper
from proxy_manager import ProxyManager

# Configuration
NUM_DAGS = 10   # 10 DAGs seulement
CSV_FILE = os.path.join(parent_dir, "data/enterprise.csv")
HTML_DIR = os.path.join(parent_dir, "data/html_pages")
PROGRESS_FILE = os.path.join(parent_dir, "data/dag_progress.json")
USE_PROXY = os.getenv('KBO_USE_PROXY', 'true').lower() == 'true'  # Toujours activÃ©

# Arguments par dÃ©faut
default_args = {
    'owner': 'kbo_team',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
}





def load_all_enterprises():
    """Charge toutes les entreprises depuis le CSV"""
    enterprises = []
    try:
        with open(CSV_FILE, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                enterprise_number = row.get('EnterpriseNumber') or row.get('enterprise_number') or row.get('number')
                if enterprise_number:
                    enterprises.append(enterprise_number)
    except Exception as e:
        print(f"âŒ Erreur lors de la lecture du CSV: {e}")
        return []
    
    return enterprises


def is_already_scraped(enterprise_number):
    """VÃ©rifie si une entreprise est dÃ©jÃ  scrapÃ©e"""
    output_file = os.path.join(HTML_DIR, f"{enterprise_number}.html")
    return os.path.exists(output_file)


def is_being_scraped(enterprise_number):
    """VÃ©rifie si une entreprise est en cours de scraping par un autre DAG"""
    lock_file = os.path.join(parent_dir, "data/locks", f"{enterprise_number}.lock")
    if not os.path.exists(lock_file):
        return False
    
    # VÃ©rifier si le lock est rÃ©cent (moins de 5 minutes)
    try:
        import time
        file_age = time.time() - os.path.getmtime(lock_file)
        if file_age > 300:  # 5 minutes
            # Lock trop vieux, on le supprime
            os.remove(lock_file)
            return False
        return True
    except:
        return False


def lock_enterprise(enterprise_number, dag_id):
    """CrÃ©e un lock pour empÃªcher d'autres DAGs de scraper cette entreprise"""
    lock_dir = os.path.join(parent_dir, "data/locks")
    os.makedirs(lock_dir, exist_ok=True)
    
    lock_file = os.path.join(lock_dir, f"{enterprise_number}.lock")
    with open(lock_file, 'w') as f:
        f.write(f"{dag_id}\n{datetime.now().isoformat()}")


def unlock_enterprise(enterprise_number):
    """Supprime le lock d'une entreprise"""
    lock_file = os.path.join(parent_dir, "data/locks", f"{enterprise_number}.lock")
    try:
        if os.path.exists(lock_file):
            os.remove(lock_file)
    except:
        pass


def get_dag_progress(dag_id):
    """RÃ©cupÃ¨re l'index actuel pour ce DAG"""
    if not os.path.exists(PROGRESS_FILE):
        return 0
    
    try:
        with open(PROGRESS_FILE, 'r') as f:
            progress = json.load(f)
            return progress.get(dag_id, 0)
    except:
        return 0


def get_failed_count(enterprise_number):
    """RÃ©cupÃ¨re le nombre d'Ã©checs pour une entreprise"""
    failed_file = os.path.join(parent_dir, "data/failed_enterprises.json")
    if not os.path.exists(failed_file):
        return 0
    
    try:
        with open(failed_file, 'r') as f:
            failed = json.load(f)
            return failed.get(enterprise_number, 0)
    except:
        return 0


def mark_enterprise_failed(enterprise_number):
    """Marque une entreprise comme Ã©chouÃ©e et retourne le nombre total d'Ã©checs"""
    failed_file = os.path.join(parent_dir, "data/failed_enterprises.json")
    failed = {}
    
    if os.path.exists(failed_file):
        try:
            with open(failed_file, 'r') as f:
                failed = json.load(f)
        except:
            pass
    
    failed[enterprise_number] = failed.get(enterprise_number, 0) + 1
    
    os.makedirs(os.path.dirname(failed_file), exist_ok=True)
    with open(failed_file, 'w') as f:
        json.dump(failed, f, indent=2)
    
    return failed[enterprise_number]


def set_dag_progress(dag_id, index):
    """Sauvegarde l'index actuel pour ce DAG"""
    progress = {}
    if os.path.exists(PROGRESS_FILE):
        try:
            with open(PROGRESS_FILE, 'r') as f:
                progress = json.load(f)
        except:
            pass
    
    progress[dag_id] = index
    
    os.makedirs(os.path.dirname(PROGRESS_FILE), exist_ok=True)
    with open(PROGRESS_FILE, 'w') as f:
        json.dump(progress, f, indent=2)


def get_next_enterprise_for_dag(dag_id):
    """
    RÃ©cupÃ¨re la prochaine entreprise Ã  scraper pour ce DAG
    Commence Ã  l'index sauvegardÃ©, cherche la premiÃ¨re non-scrapÃ©e
    Ignore les entreprises qui ont Ã©chouÃ© plus de 3 fois
    """
    all_enterprises = load_all_enterprises()
    
    if not all_enterprises:
        print(f"âŒ Aucune entreprise trouvÃ©e dans le CSV")
        return None
    
    # RÃ©cupÃ©rer l'index actuel de ce DAG
    start_index = get_dag_progress(dag_id)
    
    # Chercher la prochaine entreprise non-scrapÃ©e
    for i in range(start_index, len(all_enterprises)):
        enterprise = all_enterprises[i]
        
        # VÃ©rifier si dÃ©jÃ  scrapÃ©e
        if is_already_scraped(enterprise):
            continue
        
        # VÃ©rifier si en cours de scraping par un autre DAG
        if is_being_scraped(enterprise):
            print(f"â­ï¸  {enterprise} est en cours de scraping par un autre DAG")
            continue
        
        # VÃ©rifier si elle a dÃ©jÃ  Ã©chouÃ© trop de fois
        failed_count = get_failed_count(enterprise)
        if failed_count >= 3:
            print(f"â­ï¸  {enterprise} a Ã©chouÃ© {failed_count} fois, passage Ã  la suivante")
            continue
        
        print(f"ðŸ“‹ DAG {dag_id}: Entreprise {enterprise} (index {i}/{len(all_enterprises)}, Ã©checs: {failed_count})")
        return (enterprise, i)
    
    # Si on arrive ici, toutes les entreprises sont scrapÃ©es ou ont trop Ã©chouÃ©
    print(f"âœ… DAG {dag_id}: Toutes les entreprises accessibles sont scrapÃ©es !")
    return None



def fetch_proxies_task():
    """TÃ¢che commune pour rÃ©cupÃ©rer les proxies"""
    print("RÃ©cupÃ©ration des proxies...")
    output_file = os.path.join(parent_dir, "proxies_list.txt")
    proxies = fetch_all_proxies(output_file)
    print(f"Total de {len(proxies)} proxies rÃ©cupÃ©rÃ©s")
    return len(proxies)


def scrape_single_enterprise_task(dag_id):
    """
    TÃ¢che simple : scrape UNE entreprise, puis termine
    Ã€ la prochaine exÃ©cution, prendra l'entreprise suivante
    """
    print(f"\n{'='*60}")
    print(f"ðŸš€ {dag_id} - DÃ©marrage")
    print(f"{'='*60}\n")
    
    # Obtenir la prochaine entreprise pour ce DAG
    result = get_next_enterprise_for_dag(dag_id)
    
    if not result:
        print(f"âœ… {dag_id}: Rien Ã  faire (tout est scrapÃ©)")
        return {
            'dag_id': dag_id,
            'status': 'no_work',
            'enterprise': None
        }
    
    enterprise_number, index = result  # DÃ©baller le tuple
    
    # LOCK l'entreprise pour Ã©viter que d'autres DAGs la prennent
    lock_enterprise(enterprise_number, dag_id)
    
    print(f"ðŸŽ¯ Scraping de l'entreprise : {enterprise_number}")
    
    # Initialiser le scraper
    if USE_PROXY:
        proxy_manager = ProxyManager(
            proxy_file=os.path.join(parent_dir, "proxies_list.txt"),
            max_concurrent=20,
            request_delay=20,
            cooldown_time=300
        )
        scraper = KBOScraper(
            proxy_manager=proxy_manager,
            output_dir=HTML_DIR,
            use_proxy=True
        )
    else:
        scraper = KBOScraper(
            output_dir=HTML_DIR,
            use_proxy=False
        )
    
    # Scraper l'entreprise
    success = scraper.scrape_enterprise(enterprise_number)
    
    # UNLOCK l'entreprise dans tous les cas
    unlock_enterprise(enterprise_number)
    
    # Sauvegarder la progression selon le rÃ©sultat
    if success:
        set_dag_progress(dag_id, index + 1)
        print(f"âœ… {dag_id}: {enterprise_number} scrapÃ© avec succÃ¨s - index avancÃ© Ã  {index + 1}")
    else:
        # Marquer comme Ã©chouÃ©e
        fail_count = mark_enterprise_failed(enterprise_number)
        
        if fail_count >= 3:
            # AprÃ¨s 3 Ã©checs, on passe Ã  la suivante
            set_dag_progress(dag_id, index + 1)
            print(f"âŒ {dag_id}: {enterprise_number} Ã©chec #{fail_count} - ABANDONNÃ‰, passage Ã  la suivante")
        else:
            # Moins de 3 Ã©checs, on garde le mÃªme index pour retry
            print(f"âŒ {dag_id}: {enterprise_number} Ã©chec #{fail_count}/3 - retry Ã  la prochaine exÃ©cution")
    
    # RÃ©sultat
    print(f"{'='*60}\n")
    
    return {
        'dag_id': dag_id,
        'status': 'success' if success else 'failed',
        'enterprise': enterprise_number,
        'index': index
    }




# ============================================================================
# GÃ‰NÃ‰RATION DES DAGs
# ============================================================================

print("\n" + "="*70)
print(f"ðŸš€ GÃ‰NÃ‰RATION DES DAGs DE SCRAPING")
print("="*70)

# VÃ©rifier l'Ã©tat
all_enterprises = load_all_enterprises()
print(f"ðŸ“Š Total entreprises dans CSV : {len(all_enterprises):,}")
print(f"ðŸ”§ Nombre de DAGs de scraping : {NUM_DAGS}")
print("="*70)

# ============================================================================
# DAG 0 : Fetch Proxies (manuel, Ã  exÃ©cuter une seule fois)
# ============================================================================

with DAG(
    'kbo_fetch_proxies',
    default_args=default_args,
    description='RÃ©cupÃ¨re les proxies - Ã€ exÃ©cuter manuellement une fois',
    schedule=None,  # Manuel uniquement
    start_date=datetime(2025, 1, 1),
    catchup=False,
    tags=['kbo', 'setup', 'proxies'],
    max_active_runs=1,
) as dag_proxies:
    
    task_fetch_proxies = PythonOperator(
        task_id='fetch_proxies',
        python_callable=fetch_proxies_task,
    )

globals()['kbo_fetch_proxies'] = dag_proxies

# ============================================================================
# DAGs 1-10 : Scraping en continu
# ============================================================================

# CrÃ©er les 10 DAGs
for dag_num in range(1, NUM_DAGS + 1):
    dag_id = f"kbo_scraping_dag_{dag_num}"
    
    with DAG(
        dag_id,
        default_args=default_args,
        description=f'DAG {dag_num} - Scrape 1 entreprise et se relance',
        schedule=None,  # Pas de schedule automatique, se dÃ©clenche lui-mÃªme
        start_date=datetime(2025, 1, 1),
        catchup=False,
        tags=['kbo', 'scraping', 'auto', f'dag_{dag_num}'],
        max_active_runs=1,
    ) as dag:
        
        # TÃ¢che : Scraper 1 entreprise
        task_scrape = PythonOperator(
            task_id='scrape_enterprise',
            python_callable=scrape_single_enterprise_task,
            op_kwargs={
                'dag_id': dag_id
            },
        )
        
        # TÃ¢che : Relancer ce mÃªme DAG pour la prochaine entreprise
        task_trigger_next = TriggerDagRunOperator(
            task_id='trigger_next_run',
            trigger_dag_id=dag_id,  # Se dÃ©clenche lui-mÃªme
            wait_for_completion=False,
            reset_dag_run=False,
        )
        
        # Ordre d'exÃ©cution : scrape puis relance
        task_scrape >> task_trigger_next
        
        # Enregistrer le DAG
        globals()[dag_id] = dag

print(f"\nâœ… 1 DAG de setup + {NUM_DAGS} DAGs de scraping crÃ©Ã©s")
print(f"")
print(f"ðŸ“‹ Pour dÃ©marrer :")
print(f"   1. ExÃ©cuter 'kbo_fetch_proxies' une fois (manuel)")
print(f"   2. Activer les 10 DAGs de scraping")
print(f"   3. Cliquer 'Trigger' une fois sur chaque DAG (1 Ã  10)")
print(f"   4. Les DAGs se relanceront automatiquement aprÃ¨s chaque entreprise")
print(f"")
print(f"ðŸ”„ Mode : Auto-relance continue")
print("="*70 + "\n")
